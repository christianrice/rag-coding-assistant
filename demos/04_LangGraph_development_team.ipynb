{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade langchain langsmith langgraph langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get OPENAI_API_KEY from .env file\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_ORGANIZATION\"] = os.getenv(\"OPENAI_ORGANIZATION\")\n",
    "\n",
    "# Initialize LangSmith\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Agent Demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate import Client\n",
    "\n",
    "client = Client(\"http://localhost:8080\")\n",
    "\n",
    "\n",
    "def query_collection(query):\n",
    "    response = (\n",
    "        client.query.get(\"code_example\", [\"code\"])\n",
    "        .with_near_text({\"concepts\": [query]})\n",
    "        .with_limit(3)\n",
    "        .do()\n",
    "    )\n",
    "\n",
    "    formatted_response = \"\"\n",
    "    for result in response[\"data\"][\"Get\"][\"Code_example\"]:\n",
    "        formatted_response += \"# Example:\\n\"\n",
    "        formatted_response += f\"{result['code']}\\n\\n\\n\"\n",
    "\n",
    "    return formatted_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "\n",
    "# Set up the system template with a variable for context\n",
    "system_template = \"\"\"\n",
    "Generate working code for a Jupyter Notebook based on the user's request. Your code should use LangChain, and specifically use LangChain's Expression Language in structuring your code.\n",
    "\n",
    "Strictly adhere to the code examples delimited by triple backticks below as context for how LangChain's API works. DO NOT use any patterns that you do not find in the example below, unless you are 100% certain they work in LangChain:\n",
    "\n",
    "```\n",
    "{context}\n",
    "```\n",
    "\n",
    "Before sharing, double check your work. I will tip you $100 if your code is perfect.\n",
    "\n",
    "Do not explain your work, just share working code.\n",
    "\"\"\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "# Set up the human template with a variable for the request\n",
    "human_template = \"\"\"\n",
    "{request}\n",
    "\"\"\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "code_writing_runnable = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set up the agent's tools\n",
    "\n",
    "tools = []\n",
    "pseudo_tools_visible = [\n",
    "    \"Retrieve Context\",\n",
    "    \"Write Code\",\n",
    "    \"Review Code\",\n",
    "    # \"Save Code\",\n",
    "]\n",
    "pseudo_tools_hidden = [\n",
    "    \"Store Request\",\n",
    "]\n",
    "\n",
    "agent_tools = tools + pseudo_tools_visible + pseudo_tools_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.pydantic_v1 import BaseModel\n",
    "from enum import Enum\n",
    "from langchain.tools import StructuredTool\n",
    "\n",
    "\n",
    "# Set the agent options, which is FINISH plus all tools, with the exception of the hidden tools\n",
    "agent_options = [\"FINISH\"] + agent_tools\n",
    "agent_options = [item for item in agent_options if item not in pseudo_tools_hidden]\n",
    "\n",
    "RouteOptions = Enum(\"RouteOptions\", {option: option for option in agent_options})\n",
    "\n",
    "\n",
    "class RouteInput(BaseModel):\n",
    "    next: RouteOptions\n",
    "\n",
    "\n",
    "def route(route: str) -> str:\n",
    "    return route\n",
    "\n",
    "\n",
    "router = StructuredTool.from_function(\n",
    "    func=route,\n",
    "    name=\"route\",\n",
    "    description=\"Select the next team member to use\",\n",
    "    args_schema=RouteInput,\n",
    "    return_direct=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "system_prompt_initial = \"\"\"\n",
    "You are a supervisor tasked with managing a development team consisting of the following members: {members}.\n",
    "\n",
    "Given the following feature request from a user, respond with the worker to act next.\n",
    "\n",
    "Each worker will perform a task and respond with their results and status. This task is complete as soon as you know a worker has retrieved context related to the user's feature request. When the task is complete, respond with FINISH.\n",
    "\n",
    "You typically follow this pattern:\n",
    "\n",
    "1) Retrieve context related to the user's query. This is a REQUIRED step before writing code\n",
    "2) Write code to solve the problem\n",
    "3) Save the code you have written once the reviewer has approved the code\n",
    "\"\"\"\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(system_prompt_initial),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ").partial(options=str(agent_options), members=\", \".join(agent_tools))\n",
    "\n",
    "# Choose the LLM that will drive the agent\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", streaming=True)\n",
    "\n",
    "# Construct the OpenAI Functions agent\n",
    "agent_runnable = create_openai_functions_agent(llm, [router], prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the Agent State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence, Union\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The list of previous messages in the conversation\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    # The outcome of a given call to the agent\n",
    "    # Needs `None` as a valid type, since this is what this will start as\n",
    "    agent_outcome: Union[AgentAction, AgentFinish, None]\n",
    "    # List of actions and corresponding observations\n",
    "    # Here we annotate this with `operator.add` to indicate that operations to\n",
    "    # this state should be ADDED to the existing values (not overwrite it)\n",
    "    intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]\n",
    "    # The user's original request\n",
    "    original_request: str\n",
    "    # The context for code writing\n",
    "    context: str\n",
    "    # The code sample being generated\n",
    "    code: str\n",
    "    # Track whether the code hsa been approved\n",
    "    code_approved: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the node actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolInvocation\n",
    "import json\n",
    "from langchain_core.messages import FunctionMessage, AIMessage\n",
    "\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state):\n",
    "    # If the agent outcome is an AgentFinish, then we return `exit` string\n",
    "    # This will be used when setting up the graph to define the flow\n",
    "    if isinstance(state[\"agent_outcome\"], AgentFinish):\n",
    "        return \"end\"\n",
    "    # Otherwise, an AgentAction is returned\n",
    "    # Here we return `continue` string\n",
    "    # This will be used when setting up the graph to define the flow\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state):\n",
    "    # messages = state['messages']\n",
    "    agent_outcome = agent_runnable.invoke(state)\n",
    "    return {\"agent_outcome\": agent_outcome}\n",
    "\n",
    "\n",
    "def call_set_initial_state(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    return {\"original_request\": last_message.content}\n",
    "\n",
    "\n",
    "# Define the function to execute tools\n",
    "def call_tool(state):\n",
    "    # We construct an ToolInvocation from the function_call\n",
    "    tool = state['agent_outcome'].tool_input['next']\n",
    "    print(\"Running Tool: \", tool)\n",
    "\n",
    "    if tool == \"Retrieve Context\":\n",
    "        print(\"Retreive Context\")\n",
    "        context = query_collection(state[\"original_request\"])\n",
    "        new_message = AIMessage(content=\"You have context now\")\n",
    "        return {\"context\": context, \"messages\": [new_message]}\n",
    "    elif tool == \"Write Code\":\n",
    "        print(\"successfully writing code now\")\n",
    "        code_writing_runnable.invoke({\"context\": state[\"context\"], \"request\": state[\"original_request\"]})\n",
    "        new_message = AIMessage(content=\"You have code now\")\n",
    "        return {\"code\": new_message, \"messages\": [new_message]}\n",
    "    elif tool == \"Review Code\":\n",
    "        print(\"successfully reviewing code now\")\n",
    "        new_message = AIMessage(content=\"Code is approved\")\n",
    "        return {\"code_approved\": True, \"messages\": [new_message]}\n",
    "    elif tool == \"Save Code\":\n",
    "        print(\"Save Code\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Define a new graph\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "graph.add_node(\"agent\", call_model)\n",
    "graph.add_node(\"action\", call_tool)\n",
    "graph.add_node(\"initial_state\", call_set_initial_state)\n",
    "\n",
    "# Set the entrypoint\n",
    "graph.set_entry_point(\"initial_state\")\n",
    "\n",
    "# Add a conditional edge\n",
    "graph.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"action\",\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Aadd the Normal Edges\n",
    "graph.add_edge(\"action\", \"agent\")\n",
    "graph.add_edge(\"initial_state\", \"agent\")\n",
    "\n",
    "# Compile it\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'initial_state':\n",
      "---\n",
      "{'original_request': '\\nCreate a chain that does the following:\\n- Accept a string named answer as input\\n- Format a System and Human message using templates. The System message has output instructions via Pydantic. The Human message uses answer as context. Output instructions should require format to a Pydantic schema for hmw_question with a question (up to 10 words) and a role (either CMO, CTO, or CEO) \\n- Pass the messages to OpenAI\\n- Parse the response using Pydantic\\n'}\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'agent':\n",
      "---\n",
      "{'agent_outcome': AgentActionMessageLog(tool='route', tool_input={'next': 'Retrieve Context'}, log=\"\\nInvoking: `route` with `{'next': 'Retrieve Context'}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"next\":\"Retrieve Context\"}', 'name': 'route'}})])}\n",
      "\n",
      "---\n",
      "\n",
      "Running Tool:  Retrieve Context\n",
      "Retreive Context\n",
      "Output from node 'action':\n",
      "---\n",
      "{'context': '# Example:\\n# Create a chain that does the following and streams the response:\\n# - Accept nothing as input\\n# - Format messages from System and Human as a prompt\\n# - Pass messages to OpenAI\\n# - Parse the OpenAI response as a string\\n# - Stream the response\\n\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.schema.messages import HumanMessage, SystemMessage\\nfrom langchain.schema.output_parser import StrOutputParser\\n\\n# Generate system and human messages\\nmessages = [\\n    SystemMessage(content=\"You\\'re a helpful assistant\"),\\n    HumanMessage(content=\"What is the purpose of model regularization?\"),\\n]\\n\\nprompt = ChatPromptTemplate.from_messages(messages)\\nmodel = ChatOpenAI()\\noutput_parser = StrOutputParser()\\n\\nchain = prompt | model | output_parser\\n\\n# Stream the chain\\nfor chunk in chain.stream({}):\\n    print(chunk, end=\"\", flush=True)\\n\\n\\n# Example:\\n# Create a chain that does the following:\\n# - Accept a string as input\\n# - Retrieve matching documents from a DocArrayInMemorySearch vectorstore, and pass through the results and the original question to a prompt\\n# - Format the prompt using variables from the object\\n# - Pass the prompt to OpenAI\\n# - Parse the OpenAI response as a string\\n\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.schema.output_parser import StrOutputParser\\nfrom langchain.schema.runnable import RunnableParallel, RunnablePassthrough\\nfrom langchain.vectorstores import DocArrayInMemorySearch\\n\\nvectorstore = DocArrayInMemorySearch.from_texts(\\n    [\"Tom likes clouds\", \"bears like honey\"],\\n    embedding=OpenAIEmbeddings(),\\n)\\nretriever = vectorstore.as_retriever()\\n\\ntemplate = \"\"\"Answer the question:\\n{context}\\n\\nQuestion: {question}\\n\"\"\"\\nprompt = ChatPromptTemplate.from_template(template)\\nmodel = ChatOpenAI()\\noutput_parser = StrOutputParser()\\n\\nsetup_and_retrieval = RunnableParallel(\\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\\n)\\n\\nchain = setup_and_retrieval | prompt | model | output_parser\\n\\nchain.invoke(\"what does Tom like?\")\\n\\n\\n', 'messages': [AIMessage(content='You have context now')]}\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'agent':\n",
      "---\n",
      "{'agent_outcome': AgentActionMessageLog(tool='route', tool_input={'next': 'Write Code'}, log=\"\\nInvoking: `route` with `{'next': 'Write Code'}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"next\":\"Write Code\"}', 'name': 'route'}})])}\n",
      "\n",
      "---\n",
      "\n",
      "Running Tool:  Write Code\n",
      "successfully writing code now\n",
      "Output from node 'action':\n",
      "---\n",
      "{'code': AIMessage(content='You have code now'), 'messages': [AIMessage(content='You have code now')]}\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'agent':\n",
      "---\n",
      "{'agent_outcome': AgentFinish(return_values={'output': 'FINISH'}, log='FINISH')}\n",
      "\n",
      "---\n",
      "\n",
      "Output from node '__end__':\n",
      "---\n",
      "{'messages': [HumanMessage(content='\\nCreate a chain that does the following:\\n- Accept a string named answer as input\\n- Format a System and Human message using templates. The System message has output instructions via Pydantic. The Human message uses answer as context. Output instructions should require format to a Pydantic schema for hmw_question with a question (up to 10 words) and a role (either CMO, CTO, or CEO) \\n- Pass the messages to OpenAI\\n- Parse the response using Pydantic\\n'), AIMessage(content='You have context now'), AIMessage(content='You have code now')], 'agent_outcome': AgentFinish(return_values={'output': 'FINISH'}, log='FINISH'), 'intermediate_steps': [], 'original_request': '\\nCreate a chain that does the following:\\n- Accept a string named answer as input\\n- Format a System and Human message using templates. The System message has output instructions via Pydantic. The Human message uses answer as context. Output instructions should require format to a Pydantic schema for hmw_question with a question (up to 10 words) and a role (either CMO, CTO, or CEO) \\n- Pass the messages to OpenAI\\n- Parse the response using Pydantic\\n', 'context': '# Example:\\n# Create a chain that does the following and streams the response:\\n# - Accept nothing as input\\n# - Format messages from System and Human as a prompt\\n# - Pass messages to OpenAI\\n# - Parse the OpenAI response as a string\\n# - Stream the response\\n\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.schema.messages import HumanMessage, SystemMessage\\nfrom langchain.schema.output_parser import StrOutputParser\\n\\n# Generate system and human messages\\nmessages = [\\n    SystemMessage(content=\"You\\'re a helpful assistant\"),\\n    HumanMessage(content=\"What is the purpose of model regularization?\"),\\n]\\n\\nprompt = ChatPromptTemplate.from_messages(messages)\\nmodel = ChatOpenAI()\\noutput_parser = StrOutputParser()\\n\\nchain = prompt | model | output_parser\\n\\n# Stream the chain\\nfor chunk in chain.stream({}):\\n    print(chunk, end=\"\", flush=True)\\n\\n\\n# Example:\\n# Create a chain that does the following:\\n# - Accept a string as input\\n# - Retrieve matching documents from a DocArrayInMemorySearch vectorstore, and pass through the results and the original question to a prompt\\n# - Format the prompt using variables from the object\\n# - Pass the prompt to OpenAI\\n# - Parse the OpenAI response as a string\\n\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.schema.output_parser import StrOutputParser\\nfrom langchain.schema.runnable import RunnableParallel, RunnablePassthrough\\nfrom langchain.vectorstores import DocArrayInMemorySearch\\n\\nvectorstore = DocArrayInMemorySearch.from_texts(\\n    [\"Tom likes clouds\", \"bears like honey\"],\\n    embedding=OpenAIEmbeddings(),\\n)\\nretriever = vectorstore.as_retriever()\\n\\ntemplate = \"\"\"Answer the question:\\n{context}\\n\\nQuestion: {question}\\n\"\"\"\\nprompt = ChatPromptTemplate.from_template(template)\\nmodel = ChatOpenAI()\\noutput_parser = StrOutputParser()\\n\\nsetup_and_retrieval = RunnableParallel(\\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\\n)\\n\\nchain = setup_and_retrieval | prompt | model | output_parser\\n\\nchain.invoke(\"what does Tom like?\")\\n\\n\\n', 'code': AIMessage(content='You have code now'), 'code_approved': None}\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "feature_request = \"\"\"\n",
    "Create a chain that does the following:\n",
    "- Accept a string named answer as input\n",
    "- Format a System and Human message using templates. The System message has output instructions via Pydantic. The Human message uses answer as context. Output instructions should require format to a Pydantic schema for hmw_question with a question (up to 10 words) and a role (either CMO, CTO, or CEO) \n",
    "- Pass the messages to OpenAI\n",
    "- Parse the response using Pydantic\n",
    "\"\"\"\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=feature_request)]}\n",
    "for output in app.stream(inputs):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
