{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get OPENAI_API_KEY from .env file\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"OPENAI_ORGANIZATION\"] = os.getenv('OPENAI_ORGANIZATION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate import Client\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "\n",
    "# Assuming 'client' is a Weaviate client that has been initialized\n",
    "client = Client(\"http://localhost:8080\")\n",
    "\n",
    "def query_collection(query):\n",
    "    response = (\n",
    "        client.query\n",
    "        .get(\"code_example\", [\"code\"])\n",
    "        .with_near_text({\n",
    "            \"concepts\": [query]\n",
    "        })\n",
    "        .with_limit(4)\n",
    "        .with_additional([\"distance\"])\n",
    "        .do()\n",
    "    )\n",
    "    return response\n",
    "\n",
    "system_template = \"\"\"\n",
    "Generate working code for a Jupyter Notebook based on the user's request. Your code should use LangChain, and specifically use LangChains's Expression Language in structuring your code. Strictly adhere to the code examples delimited by triple backticks below as context for how LangChain's API works. DO NOT use any patterns that you do not find in the example below, unless you are 100% certain they work in LangChain:\n",
    "\n",
    "```\n",
    "{context}\n",
    "```\n",
    "\n",
    "Before sharing, double check your work. I will tip you $100 if your code is perfect.\n",
    "\n",
    "Do not explain your work, just share working code.\n",
    "\"\"\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "human_template = \"\"\"\n",
    "{request}\n",
    "\"\"\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": RunnableLambda(query_collection), \"request\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "model = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = setup_and_retrieval | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"\"\"\n",
    "# Create a chain that does the following:\n",
    "# - Accept a string as input\n",
    "# - Format message from System the input to send to OpenAI. The prompt has output instructions using Pydantic that expects a schema of a list of Experts with name and description\n",
    "# - Pass the prompt to OpenAI\n",
    "# - Parse the response using Pydantic as a list of Experts\n",
    "\"\"\"\n",
    "\n",
    "# Stream the chain\n",
    "for chunk in chain.stream(request):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
