{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get OPENAI_API_KEY from .env file\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"OPENAI_ORGANIZATION\"] = os.getenv('OPENAI_ORGANIZATION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Weaviate client\n",
    "client = Client(\"http://localhost:8080\")\n",
    "\n",
    "def query_collection(query):\n",
    "    response = (\n",
    "        client.query\n",
    "        .get(\"code_example\", [\"code\"])\n",
    "        .with_near_text({\n",
    "            \"concepts\": [query]\n",
    "        })\n",
    "        .with_limit(9)\n",
    "        .do()\n",
    "    )\n",
    "    return response\n",
    "\n",
    "query_collection(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate import Client\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "\n",
    "# Initialize the Weaviate client\n",
    "client = Client(\"http://localhost:8080\")\n",
    "\n",
    "def query_collection(query):\n",
    "    response = (\n",
    "        client.query\n",
    "        .get(\"code_example\", [\"code\"])\n",
    "        .with_near_text({\n",
    "            \"concepts\": [query]\n",
    "        })\n",
    "        .with_limit(9)\n",
    "        .do()\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# Set up the system template with a variable for context\n",
    "system_template = \"\"\"\n",
    "Generate working code for a Jupyter Notebook based on the user's request. Your code should use LangChain, and specifically use LangChain's Expression Language in structuring your code.\n",
    "\n",
    "Strictly adhere to the code examples delimited by triple backticks below as context for how LangChain's API works. DO NOT use any patterns that you do not find in the example below, unless you are 100% certain they work in LangChain:\n",
    "\n",
    "```\n",
    "{context}\n",
    "```\n",
    "\n",
    "Before sharing, double check your work. I will tip you $100 if your code is perfect.\n",
    "\n",
    "Do not explain your work, just share working code.\n",
    "\"\"\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "# Set up the human template with a variable for the request\n",
    "human_template = \"\"\"\n",
    "{request}\n",
    "\"\"\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "# Create the elements of the chain\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": RunnableLambda(query_collection), \"request\": RunnablePassthrough()}\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "model = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = setup_and_retrieval | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "from typing import List\n",
      "\n",
      "from langchain.output_parsers import PydanticOutputParser\n",
      "from langchain.pydantic_v1 import BaseModel, Field\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain_core.runnables import RunnablePassthrough\n",
      "from langchain.prompts.chat import (\n",
      "    ChatPromptTemplate,\n",
      "    SystemMessagePromptTemplate,\n",
      "    HumanMessagePromptTemplate,\n",
      ")\n",
      "\n",
      "class HMWQuestion(BaseModel):\n",
      "    question: str = Field(description=\"A question up to 10 words\")\n",
      "    role: str = Field(description=\"Role of the person asking the question\", regex=\"^(CMO|CTO|CEO)$\")\n",
      "\n",
      "parser = PydanticOutputParser(pydantic_object=HMWQuestion)\n",
      "\n",
      "system_template = \"Please format your response according to the following instructions:\\n{format_instructions}\"\n",
      "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
      "\n",
      "human_template = \"{answer}\"\n",
      "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
      "\n",
      "prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
      "\n",
      "model = ChatOpenAI()\n",
      "\n",
      "chain = (\n",
      "    {\"answer\": RunnablePassthrough()}\n",
      "    | prompt\n",
      "    | model\n",
      "    | parser\n",
      ")\n",
      "\n",
      "chain.invoke(\"What is the most effective marketing strategy?\")\n",
      "```"
     ]
    }
   ],
   "source": [
    "request = \"\"\"\n",
    "Create a chain that does the following:\n",
    "- Accept a string named answer as input\n",
    "- Format a System and Human message using templates. The System message has output instructions via Pydantic. The Human message uses answer as context. Output instructions should require format to a Pydantic schema for hmw_question with a question (up to 10 words) and a role (either CMO, CTO, or CEO) \n",
    "- Pass the messages to OpenAI\n",
    "- Parse the response using Pydantic\n",
    "\"\"\"\n",
    "\n",
    "# Stream the chain\n",
    "for chunk in chain.stream(request):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
