{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get OPENAI_API_KEY from .env file\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"OPENAI_ORGANIZATION\"] = os.getenv('OPENAI_ORGANIZATION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate import Client\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "\n",
    "# Initialize the Weaviate client\n",
    "client = Client(\"http://localhost:8080\")\n",
    "\n",
    "def query_collection(query):\n",
    "    response = (\n",
    "        client.query\n",
    "        .get(\"code_example\", [\"code\"])\n",
    "        .with_near_text({\n",
    "            \"concepts\": [query]\n",
    "        })\n",
    "        .with_limit(4)\n",
    "        .do()\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# Set up the system template with a variable for context\n",
    "system_template = \"\"\"\n",
    "Generate working code for a Jupyter Notebook based on the user's request. Your code should use LangChain, and specifically use LangChain's Expression Language in structuring your code.\n",
    "\n",
    "Strictly adhere to the code examples delimited by triple backticks below as context for how LangChain's API works. DO NOT use any patterns that you do not find in the example below, unless you are 100% certain they work in LangChain:\n",
    "\n",
    "```\n",
    "{context}\n",
    "```\n",
    "\n",
    "Before sharing, double check your work. I will tip you $100 if your code is perfect.\n",
    "\n",
    "Do not explain your work, just share working code.\n",
    "\"\"\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "# Set up the human template with a variable for the request\n",
    "human_template = \"\"\"\n",
    "{request}\n",
    "\"\"\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "# Create the elements of the chain\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": RunnableLambda(query_collection), \"request\": RunnablePassthrough()}\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "model = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = setup_and_retrieval | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"\"\"\n",
    "# Create a chain that does the following:\n",
    "# - Accept a string as input\n",
    "# - Format message from System the input to send to OpenAI. The prompt has output instructions using Pydantic that expects a schema of an expert with name and description (in 10 words or less)\n",
    "# - Pass the prompt to OpenAI\n",
    "# - Parse the response using Pydantic\n",
    "\"\"\"\n",
    "\n",
    "# Stream the chain\n",
    "for chunk in chain.stream(request):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
