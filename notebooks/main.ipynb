{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install weaviate-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract information from code files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"file_name\": \"example_4.py\",\n",
      "  \"tags\": [\n",
      "    \"langchain\"\n",
      "  ],\n",
      "  \"description\": \"Create a chain that does the following and streams the response:\\n- Accept a string as input\\n- Format messages from System and Human as a prompt\\n- Pass messages to OpenAI\\n- Parse the OpenAI response as a string\\n\",\n",
      "  \"code\": \"from langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.schema.messages import HumanMessage, SystemMessage\\nfrom langchain.schema.output_parser import StrOutputParser\\n\\n# Generate system and human messages\\nmessages = [\\n    SystemMessage(content=\\\"You're a helpful assistant\\\"),\\n    HumanMessage(content=\\\"What is the purpose of model regularization?\\\"),\\n]\\n\\nprompt = ChatPromptTemplate.from_messages(messages)\\nmodel = ChatOpenAI()\\noutput_parser = StrOutputParser()\\n\\nchain = prompt | model | output_parser\\n\\n# Stream the chain\\nfor chunk in chain.stream({}):\\n    print(chunk, end=\\\"\\\", flush=True)\"\n",
      "}\n",
      "{\n",
      "  \"file_name\": \"example_5.py\",\n",
      "  \"tags\": [\n",
      "    \"langchain\"\n",
      "  ],\n",
      "  \"description\": \"Create a chain that does the following:\\n- Accept an object\\n- Format messages from System and Human using variables from the object\\n- Pass messages to OpenAI\\n\",\n",
      "  \"code\": \"from langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts.chat import (\\n    ChatPromptTemplate,\\n    SystemMessagePromptTemplate,\\n    HumanMessagePromptTemplate,\\n)\\n\\ntemplate=\\\"You are a helpful assistant that translates {input_language} to {output_language}.\\\"\\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\\nhuman_template=\\\"{text}\\\"\\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\\n\\nchat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\\n\\nmodel = ChatOpenAI()\\n\\nchain = chat_prompt | model\\n\\nchain.invoke({'input_language': 'English', 'output_language': 'French', 'text': 'Hello, how are you?'})\"\n",
      "}\n",
      "{\n",
      "  \"file_name\": \"example_1.py\",\n",
      "  \"tags\": [\n",
      "    \"langchain\"\n",
      "  ],\n",
      "  \"description\": \"Create a chain that performs the following steps:\\n- Accepts an object as input.\\n- Formats the prompt using variables from the object.\\n- Sends the prompt to OpenAI.\\n- Parses the response as a string.\\n\",\n",
      "  \"code\": \"from langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.schema.output_parser import StrOutputParser\\n\\nprompt = ChatPromptTemplate.from_template(\\\"tell me a short joke about {topic}\\\")\\nmodel = ChatOpenAI()\\noutput_parser = StrOutputParser()\\n\\nchain = prompt | model | output_parser\\n\\nchain.invoke({\\\"topic\\\": \\\"ice cream\\\"})\"\n",
      "}\n",
      "{\n",
      "  \"file_name\": \"example_8.py\",\n",
      "  \"tags\": [\n",
      "    \"langchain\"\n",
      "  ],\n",
      "  \"description\": \"Create a chain that does the following:\\n- Accept a string as input\\n- Format the prompt using variables from the object. The prompt has format instructions to adhere to the Actor model with a name and film_names\\n- Send the prompt to OpenAI\\n- Parse the response using Pydantic\\n\",\n",
      "  \"code\": \"from typing import List\\n\\nfrom langchain.output_parsers import PydanticOutputParser\\nfrom langchain.pydantic_v1 import BaseModel, Field\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain_core.runnables import RunnablePassthrough\\n\\nclass Actor(BaseModel):\\n    name: str = Field(description=\\\"name of an actor\\\")\\n    film_names: List[str] = Field(description=\\\"list of names of films they starred in\\\")\\n\\nparser = PydanticOutputParser(pydantic_object=Actor)\\n\\nprompt_template = \\\"\\\"\\\"\\nAnswer the user query.\\\\n\\n{format_instructions}\\\\n\\n{query}\\\\n\\n\\\"\\\"\\\"\\n\\nprompt = PromptTemplate(\\n    template=prompt_template,\\n    input_variables=[\\\"query\\\"],\\n    partial_variables={\\\"format_instructions\\\": parser.get_format_instructions()},\\n)\\n\\nmodel = ChatOpenAI()\\n\\nchain = (\\n    {\\\"query\\\": RunnablePassthrough()}\\n    | prompt\\n    | model\\n    | parser\\n)\\n\\nchain.invoke(\\\"Generate the filmography for a random actor who was in Breaking Bad.\\\")\"\n",
      "}\n",
      "{\n",
      "  \"file_name\": \"example_6.py\",\n",
      "  \"tags\": [\n",
      "    \"langchain\"\n",
      "  ],\n",
      "  \"description\": \"Create a chain that does the following:\\n- Accept a string\\n- Structure the input as an object to pass to the prompt\\n- Format the prompt using variables from the object\\n- Bind a function for joke that returns a setup and punchline, and pass the message to OpenAI\\n- Parse the response as JSON returning only the setup\\n\",\n",
      "  \"code\": \"from langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\\nfrom langchain.schema.runnable import RunnablePassthrough\\n\\nprompt = ChatPromptTemplate.from_template(\\\"tell a joke about {foo}\\\")\\n\\nmodel = ChatOpenAI(model=\\\"gpt-3.5-turbo\\\")\\n\\nfunctions = [\\n    {\\n        \\\"name\\\": \\\"joke\\\",\\n        \\\"description\\\": \\\"A joke\\\",\\n        \\\"parameters\\\": {\\n            \\\"type\\\": \\\"object\\\",\\n            \\\"properties\\\": {\\n                \\\"setup\\\": {\\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The setup for the joke\\\"},\\n                \\\"punchline\\\": {\\n                    \\\"type\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"The punchline for the joke\\\",\\n                },\\n            },\\n            \\\"required\\\": [\\\"setup\\\", \\\"punchline\\\"],\\n        },\\n    }\\n]\\n\\nchain = (\\n    {\\\"foo\\\": RunnablePassthrough()}\\n    | prompt\\n    | model.bind(function_call={\\\"name\\\": \\\"joke\\\"}, functions=functions)\\n    | JsonKeyOutputFunctionsParser(key_name=\\\"setup\\\")\\n)\\n\\nchain.invoke(\\\"bears\\\")\"\n",
      "}\n",
      "{\n",
      "  \"file_name\": \"example_2.py\",\n",
      "  \"tags\": [\n",
      "    \"langchain\"\n",
      "  ],\n",
      "  \"description\": \"Create a chain that does the following:\\n- Accept a string as input\\n- Structure the input as an object to pass to the prompt\\n- Format the prompt using variables from the object\\n- Send the prompt to OpenAI\\n- Parse the response as a string\\n\",\n",
      "  \"code\": \"from langchain.chat_models import ChatOpenAI\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.schema.output_parser import StrOutputParser\\n\\nprompt = ChatPromptTemplate.from_template(\\n    \\\"Tell me a short joke about {topic}\\\"\\n)\\noutput_parser = StrOutputParser()\\nmodel = ChatOpenAI(model=\\\"gpt-3.5-turbo\\\")\\nchain = (\\n    {\\\"topic\\\": RunnablePassthrough()} \\n    | prompt\\n    | model\\n    | output_parser\\n)\\n\\nchain.invoke(\\\"ice cream\\\")\"\n",
      "}\n",
      "{\n",
      "  \"file_name\": \"example_3.py\",\n",
      "  \"tags\": [\n",
      "    \"langchain\",\n",
      "    \"rag\"\n",
      "  ],\n",
      "  \"description\": \"Create a chain that does the following:\\n- Accept a string as input\\n- Retrieve matching documents from a DocArrayInMemorySearch vectorstore, and pass through the results and the original question to a prompt\\n- Format the prompt using variables from the object\\n- Pass the prompt to OpenAI\\n- Parse the OpenAI response as a string\\n\",\n",
      "  \"code\": \"from langchain.chat_models import ChatOpenAI\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.schema.output_parser import StrOutputParser\\nfrom langchain.schema.runnable import RunnableParallel, RunnablePassthrough\\nfrom langchain.vectorstores import DocArrayInMemorySearch\\n\\nvectorstore = DocArrayInMemorySearch.from_texts(\\n    [\\\"harrison worked at kensho\\\", \\\"bears like to eat honey\\\"],\\n    embedding=OpenAIEmbeddings(),\\n)\\nretriever = vectorstore.as_retriever()\\n\\ntemplate = \\\"\\\"\\\"Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n\\\"\\\"\\\"\\nprompt = ChatPromptTemplate.from_template(template)\\nmodel = ChatOpenAI()\\noutput_parser = StrOutputParser()\\n\\nsetup_and_retrieval = RunnableParallel(\\n    {\\\"context\\\": retriever, \\\"question\\\": RunnablePassthrough()}\\n)\\n\\nchain = setup_and_retrieval | prompt | model | output_parser\\n\\nchain.invoke(\\\"where did harrison work?\\\")\"\n",
      "}\n",
      "{\n",
      "  \"file_name\": \"example_7.py\",\n",
      "  \"tags\": [\n",
      "    \"langchain\"\n",
      "  ],\n",
      "  \"description\": \"Create multiple chains that work together to do the following:\\n- The first chain should generate a prompt, send it OpenAI, parse the response as a string, and return the response to pass through to the next chain\\n- The second chain should generate a prompt, send it OpenAI, parse the response as a string\\n- The third chain should generate a prompt, send it OpenAI, parse the response as a string\\n- Finally, the fourth chain should format a series of messages using outputs from each of the first streams as AI, Human, and System messages, send those to OpenAI, and parse the result as a string\\n\",\n",
      "  \"code\": \"from operator import itemgetter\\n\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain.schema.output_parser import StrOutputParser\\n\\n\\nplanner = (\\n    ChatPromptTemplate.from_template(\\\"Generate a brief argument about: {input}\\\")\\n    | ChatOpenAI()\\n    | StrOutputParser()\\n    | {\\\"base_response\\\": RunnablePassthrough()}\\n)\\n\\narguments_for = (\\n    ChatPromptTemplate.from_template(\\n        \\\"List 3 pros or positive aspects of {base_response}\\\"\\n    )\\n    | ChatOpenAI()\\n    | StrOutputParser()\\n)\\narguments_against = (\\n    ChatPromptTemplate.from_template(\\n        \\\"List 3 cons or negative aspects of {base_response}\\\"\\n    )\\n    | ChatOpenAI()\\n    | StrOutputParser()\\n)\\n\\nfinal_responder = (\\n    ChatPromptTemplate.from_messages(\\n        [\\n            (\\\"ai\\\", \\\"Review your original response (below), and update it based upon the pros and cons.{original_response}\\\"),\\n            (\\\"human\\\", \\\"Pros:\\\\n{results_1}\\\\n\\\\nCons:\\\\n{results_2}\\\"),\\n            (\\\"system\\\", \\\"Generate a final response given the critique\\\"),\\n        ]\\n    )\\n    | ChatOpenAI()\\n    | StrOutputParser()\\n)\\n\\nchain = (\\n    planner\\n    | {\\n        \\\"results_1\\\": arguments_for,\\n        \\\"results_2\\\": arguments_against,\\n        \\\"original_response\\\": itemgetter(\\\"base_response\\\"),\\n    }\\n    | final_responder\\n)\\n\\nchain.invoke({\\\"input\\\": \\\"scrum\\\"})\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "# Define the path to the data directory\n",
    "DATA_DIR = \"../data\"\n",
    "\n",
    "# Define a function to extract tags, description, and code from a file\n",
    "def extract_information(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Extract the docstring\n",
    "    docstring_start = content.find('\"\"\"') + 3\n",
    "    docstring_end = content.find('\"\"\"', docstring_start)\n",
    "    docstring = content[docstring_start:docstring_end]\n",
    "\n",
    "    # Parse the docstring as YAML\n",
    "    metadata = yaml.safe_load(docstring)\n",
    "\n",
    "    # Extract tags and description\n",
    "    tags = metadata['tags']\n",
    "    description = metadata['description']\n",
    "\n",
    "    # Extract code\n",
    "    code_start = content.find('\"\"\"', docstring_end) + len('\"\"\"')\n",
    "    if code_start != -1:\n",
    "        code = content[code_start:].strip()\n",
    "    else:\n",
    "        print(f\"Error in file {file_path}: No code found after docstring.\")\n",
    "        return\n",
    "\n",
    "    return {\n",
    "        'file_name': os.path.basename(file_path),\n",
    "        'tags': tags,\n",
    "        'description': description,\n",
    "        'code': code\n",
    "    }\n",
    "\n",
    "# Define the main function to iterate through the files and extract information\n",
    "def main():\n",
    "    for file_name in os.listdir(DATA_DIR):\n",
    "        # Check if the file is a Python file and not a subdirectory\n",
    "        if file_name.endswith('.py') and os.path.isfile(os.path.join(DATA_DIR, file_name)):\n",
    "            file_path = os.path.join(DATA_DIR, file_name)\n",
    "            file_data = extract_information(file_path)\n",
    "            print(json.dumps(file_data, indent=2))\n",
    "\n",
    "# Run the main function\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate import Client\n",
    "\n",
    "# Initialize Weaviate client\n",
    "client = Client(\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_schema():\n",
    "    \"\"\"\n",
    "    Creates the schema for the 'code_example' class in Weaviate.\n",
    "    \"\"\"\n",
    "    class_obj = {\n",
    "        \"class\": \"code_example\",\n",
    "        \"vectorizer\": \"text2vec-openai\",\n",
    "        \"moduleConfig\": {\n",
    "            \"text2vec-openai\": {},\n",
    "            \"generative-openai\": {}\n",
    "        },\n",
    "        \"properties\": [\n",
    "            {\n",
    "                \"name\": \"tags\",\n",
    "                \"dataType\": [\"string[]\"],\n",
    "                \"description\": \"Tags associated with the code\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"description\",\n",
    "                \"dataType\": [\"text\"],\n",
    "                \"description\": \"Description of the code\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"code\",\n",
    "                \"dataType\": [\"text\"],\n",
    "                \"description\": \"The actual code\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    client.schema.delete_all()  # Clear existing schema\n",
    "    client.schema.create_class(class_obj)  # Create the new class\n",
    "\n",
    "# Create the schema\n",
    "create_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'Get': {'Code_example': []}}}\n"
     ]
    }
   ],
   "source": [
    "def query_collection():\n",
    "    response = (\n",
    "        client.query\n",
    "        .get(\"code_example\", [\"description\", \"code\"])\n",
    "        .with_limit(2)\n",
    "        .do()\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# Query the collection (should return 0 results initially)\n",
    "query_result = query_collection()\n",
    "print(query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate the weaviate database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_weaviate(documents):\n",
    "    \"\"\"\n",
    "    Populates the Weaviate database with the provided documents.\n",
    "    Clears the database if it is already populated.\n",
    "    \"\"\"\n",
    "    # Check if the database is already populated\n",
    "    if client.data_object.get(\"code_example\", limit=1)[\"total\"] > 0:\n",
    "        # Clear the database\n",
    "        client.schema.delete_all()\n",
    "\n",
    "    # Add each document to the Weaviate database\n",
    "    for doc in documents:\n",
    "        try:\n",
    "            # Embed description using OpenAI\n",
    "            description_embedding = client.batch.openai.embed([doc[\"description\"]])[0]\n",
    "            client.data_object.create(\n",
    "                data_object=doc,\n",
    "                class_name=\"code_example\",\n",
    "                vector=description_embedding,\n",
    "                uuid=str(uuid.uuid4())\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding document to Weaviate: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files():\n",
    "    \"\"\"\n",
    "    Modified function to process files and populate Weaviate database.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for file_name in os.listdir(DATA_DIR):\n",
    "        if file_name.endswith('.py'):\n",
    "            file_path = os.path.join(DATA_DIR, file_name)\n",
    "            data = extract_information(file_path)\n",
    "            \n",
    "            if data:\n",
    "                documents.append({\"file_name\": file_name, **data})\n",
    "            else:\n",
    "                log_error(file_name, \"tags, description, or code\")\n",
    "\n",
    "    # Populate Weaviate with extracted documents\n",
    "    populate_weaviate(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Not valid 'uuid' or 'uuid' can not be extracted from value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/crice/code/botany/coding-bot/notebooks/main.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/crice/code/botany/coding-bot/notebooks/main.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Run the main function\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/crice/code/botany/coding-bot/notebooks/main.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m process_files()\n",
      "\u001b[1;32m/Users/crice/code/botany/coding-bot/notebooks/main.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/crice/code/botany/coding-bot/notebooks/main.ipynb#X31sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m             log_error(file_name, \u001b[39m\"\u001b[39m\u001b[39mtags, description, or code\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/crice/code/botany/coding-bot/notebooks/main.ipynb#X31sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Populate Weaviate with extracted documents\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/crice/code/botany/coding-bot/notebooks/main.ipynb#X31sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m populate_weaviate(documents)\n",
      "\u001b[1;32m/Users/crice/code/botany/coding-bot/notebooks/main.ipynb Cell 7\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/crice/code/botany/coding-bot/notebooks/main.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/crice/code/botany/coding-bot/notebooks/main.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mPopulates the Weaviate database with the provided documents.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/crice/code/botany/coding-bot/notebooks/main.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mClears the database if it is already populated.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/crice/code/botany/coding-bot/notebooks/main.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/crice/code/botany/coding-bot/notebooks/main.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Check if the database is already populated\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/crice/code/botany/coding-bot/notebooks/main.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mif\u001b[39;00m client\u001b[39m.\u001b[39;49mdata_object\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcode_example\u001b[39;49m\u001b[39m\"\u001b[39;49m, limit\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)[\u001b[39m\"\u001b[39m\u001b[39mtotal\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/crice/code/botany/coding-bot/notebooks/main.ipynb#X31sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# Clear the database\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/crice/code/botany/coding-bot/notebooks/main.ipynb#X31sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     client\u001b[39m.\u001b[39mschema\u001b[39m.\u001b[39mdelete_all()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/crice/code/botany/coding-bot/notebooks/main.ipynb#X31sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Add each document to the Weaviate database\u001b[39;00m\n",
      "File \u001b[0;32m~/code/botany/stable-diffusion-webui/venv/lib/python3.11/site-packages/weaviate/data/crud_data.py:583\u001b[0m, in \u001b[0;36mDataObject.get\u001b[0;34m(self, uuid, additional_properties, with_vector, class_name, node_name, consistency_level, limit, after, offset, sort, tenant)\u001b[0m\n\u001b[1;32m    580\u001b[0m     path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/objects\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m \u001b[39mif\u001b[39;00m uuid \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 583\u001b[0m     path \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m get_valid_uuid(uuid)\n\u001b[1;32m    585\u001b[0m \u001b[39mif\u001b[39;00m consistency_level \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    586\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39mconsistency_level\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m ConsistencyLevel(consistency_level)\u001b[39m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/code/botany/stable-diffusion-webui/venv/lib/python3.11/site-packages/weaviate/util.py:372\u001b[0m, in \u001b[0;36mget_valid_uuid\u001b[0;34m(uuid)\u001b[0m\n\u001b[1;32m    370\u001b[0m     _uuid \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(uuid_lib\u001b[39m.\u001b[39mUUID(_uuid))\n\u001b[1;32m    371\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m--> 372\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNot valid \u001b[39m\u001b[39m'\u001b[39m\u001b[39muuid\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39muuid\u001b[39m\u001b[39m'\u001b[39m\u001b[39m can not be extracted from value\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[39mreturn\u001b[39;00m _uuid\n",
      "\u001b[0;31mValueError\u001b[0m: Not valid 'uuid' or 'uuid' can not be extracted from value"
     ]
    }
   ],
   "source": [
    "# Run the main function\n",
    "process_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
