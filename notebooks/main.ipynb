{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install weaviate-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate import Client\n",
    "\n",
    "# Initialize Weaviate client\n",
    "client = Client(\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purge_weaviate_db():\n",
    "    # Assuming class name is \"CodeExample\"\n",
    "    # This will delete all objects of the class \"CodeExample\"\n",
    "    client.data_object.delete(class_name=\"CodeExample\", where=\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def get_embeddings(text):\n",
    "    response = openai.Embedding.create(input=text, engine=\"text-similarity-babbage-001\")\n",
    "    return response[\"data\"][0][\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/example_4.py\n",
      "<re.Match object; span=(0, 270), match='\"\"\"\\ntags: [langchain]\\ndescription: |\\n    Creat>\n",
      "{\n",
      "  \"file_name\": \"example_4.py\",\n",
      "  \"tags\": [\n",
      "    \"langchain\"\n",
      "  ],\n",
      "  \"description\": \"Create a chain that does the following and streams the response:\\n- Accept a string as input\\n- Format messages from System and Human as a prompt\\n- Pass messages to OpenAI\\n- Parse the OpenAI response as a string\\n\",\n",
      "  \"code\": \"tags: [langchain]\\ndescription: |\\n    Create a chain that does the following and streams the response:\\n    - Accept a string as input\\n    - Format messages from System and Human as a prompt\\n    - Pass messages to OpenAI\\n    - Parse the OpenAI response as a string\\n\\\"\\\"\\\"\\n\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.schema.messages import HumanMessage, SystemMessage\\nfrom langchain.schema.output_parser import StrOutputParser\\n\\n# Generate system and human messages\\nmessages = [\\n    SystemMessage(content=\\\"You're a helpful assistant\\\"),\\n    HumanMessage(content=\\\"What is the purpose of model regularization?\\\"),\\n]\\n\\nprompt = ChatPromptTemplate.from_messages(messages)\\nmodel = ChatOpenAI()\\noutput_parser = StrOutputParser()\\n\\nchain = prompt | model | output_parser\\n\\n# Stream the chain\\nfor chunk in chain.stream({}):\\n    print(chunk, end=\\\"\\\", flush=True)\"\n",
      "}\n",
      "../data/example_5.py\n",
      "<re.Match object; span=(0, 213), match='\"\"\"\\ntags: [langchain]\\ndescription: |\\n    Creat>\n",
      "{\n",
      "  \"file_name\": \"example_5.py\",\n",
      "  \"tags\": [\n",
      "    \"langchain\"\n",
      "  ],\n",
      "  \"description\": \"Create a chain that does the following:\\n- Accept an object\\n- Format messages from System and Human using variables from the object\\n- Pass messages to OpenAI\\n\",\n",
      "  \"code\": \"tags: [langchain]\\ndescription: |\\n    Create a chain that does the following:\\n    - Accept an object\\n    - Format messages from System and Human using variables from the object\\n    - Pass messages to OpenAI\\n\\\"\\\"\\\"\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts.chat import (\\n    ChatPromptTemplate,\\n    SystemMessagePromptTemplate,\\n    HumanMessagePromptTemplate,\\n)\\n\\ntemplate=\\\"You are a helpful assistant that translates {input_language} to {output_language}.\\\"\\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\\nhuman_template=\\\"{text}\\\"\\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\\n\\nchat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\\n\\nmodel = ChatOpenAI()\\n\\nchain = chat_prompt | model\\n\\nchain.invoke({'input_language': 'English', 'output_language': 'French', 'text': 'Hello, how are you?'})\"\n",
      "}\n",
      "../data/example_1.py\n",
      "<re.Match object; span=(0, 259), match='\"\"\"\\ntags: [langchain]\\ndescription: |\\n    Creat>\n",
      "{\n",
      "  \"file_name\": \"example_1.py\",\n",
      "  \"tags\": [\n",
      "    \"langchain\"\n",
      "  ],\n",
      "  \"description\": \"Create a chain that performs the following steps:\\n- Accepts an object as input.\\n- Formats the prompt using variables from the object.\\n- Sends the prompt to OpenAI.\\n- Parses the response as a string.\\n\",\n",
      "  \"code\": \"tags: [langchain]\\ndescription: |\\n    Create a chain that performs the following steps:\\n    - Accepts an object as input.\\n    - Formats the prompt using variables from the object.\\n    - Sends the prompt to OpenAI.\\n    - Parses the response as a string.\\n\\\"\\\"\\\"\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.schema.output_parser import StrOutputParser\\n\\nprompt = ChatPromptTemplate.from_template(\\\"tell me a short joke about {topic}\\\")\\nmodel = ChatOpenAI()\\noutput_parser = StrOutputParser()\\n\\nchain = prompt | model | output_parser\\n\\nchain.invoke({\\\"topic\\\": \\\"ice cream\\\"})\"\n",
      "}\n",
      "../data/example_6.py\n",
      "<re.Match object; span=(0, 379), match='\"\"\"\\ntags: [langchain]\\ndescription: |\\n    Creat>\n",
      "{\n",
      "  \"file_name\": \"example_6.py\",\n",
      "  \"tags\": [\n",
      "    \"langchain\"\n",
      "  ],\n",
      "  \"description\": \"Create a chain that does the following:\\n- Accept a string\\n- Structure the input as an object to pass to the prompt\\n- Format the prompt using variables from the object\\n- Bind a function for joke that returns a setup and punchline, and pass the message to OpenAI\\n- Parse the response as JSON returning only the setup\\n\",\n",
      "  \"code\": \"tags: [langchain]\\ndescription: |\\n    Create a chain that does the following:\\n    - Accept a string\\n    - Structure the input as an object to pass to the prompt\\n    - Format the prompt using variables from the object\\n    - Bind a function for joke that returns a setup and punchline, and pass the message to OpenAI\\n    - Parse the response as JSON returning only the setup\\n\\\"\\\"\\\"\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\\nfrom langchain.schema.runnable import RunnablePassthrough\\n\\nprompt = ChatPromptTemplate.from_template(\\\"tell a joke about {foo}\\\")\\n\\nmodel = ChatOpenAI(model=\\\"gpt-3.5-turbo\\\")\\n\\nfunctions = [\\n    {\\n        \\\"name\\\": \\\"joke\\\",\\n        \\\"description\\\": \\\"A joke\\\",\\n        \\\"parameters\\\": {\\n            \\\"type\\\": \\\"object\\\",\\n            \\\"properties\\\": {\\n                \\\"setup\\\": {\\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The setup for the joke\\\"},\\n                \\\"punchline\\\": {\\n                    \\\"type\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"The punchline for the joke\\\",\\n                },\\n            },\\n            \\\"required\\\": [\\\"setup\\\", \\\"punchline\\\"],\\n        },\\n    }\\n]\\n\\nchain = (\\n    {\\\"foo\\\": RunnablePassthrough()}\\n    | prompt\\n    | model.bind(function_call={\\\"name\\\": \\\"joke\\\"}, functions=functions)\\n    | JsonKeyOutputFunctionsParser(key_name=\\\"setup\\\")\\n)\\n\\nchain.invoke(\\\"bears\\\")\"\n",
      "}\n",
      "../data/example_2.py\n",
      "<re.Match object; span=(0, 301), match='\"\"\"\\ntags: [langchain]\\ndescription: |\\n    Creat>\n",
      "{\n",
      "  \"file_name\": \"example_2.py\",\n",
      "  \"tags\": [\n",
      "    \"langchain\"\n",
      "  ],\n",
      "  \"description\": \"Create a chain that does the following:\\n- Accept a string as input\\n- Structure the input as an object to pass to the prompt\\n- Format the prompt using variables from the object\\n- Send the prompt to OpenAI\\n- Parse the response as a string\\n\",\n",
      "  \"code\": \"tags: [langchain]\\ndescription: |\\n    Create a chain that does the following:\\n    - Accept a string as input\\n    - Structure the input as an object to pass to the prompt\\n    - Format the prompt using variables from the object\\n    - Send the prompt to OpenAI\\n    - Parse the response as a string\\n\\\"\\\"\\\"\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.schema.output_parser import StrOutputParser\\n\\nprompt = ChatPromptTemplate.from_template(\\n    \\\"Tell me a short joke about {topic}\\\"\\n)\\noutput_parser = StrOutputParser()\\nmodel = ChatOpenAI(model=\\\"gpt-3.5-turbo\\\")\\nchain = (\\n    {\\\"topic\\\": RunnablePassthrough()} \\n    | prompt\\n    | model\\n    | output_parser\\n)\\n\\nchain.invoke(\\\"ice cream\\\")\"\n",
      "}\n",
      "../data/example_3.py\n",
      "<re.Match object; span=(0, 396), match='\"\"\"\\ntags: [langchain, rag]\\ndescription: |\\n    >\n",
      "{\n",
      "  \"file_name\": \"example_3.py\",\n",
      "  \"tags\": [\n",
      "    \"langchain\",\n",
      "    \"rag\"\n",
      "  ],\n",
      "  \"description\": \"Create a chain that does the following:\\n- Accept a string as input\\n- Retrieve matching documents from a DocArrayInMemorySearch vectorstore, and pass through the results and the original question to a prompt\\n- Format the prompt using variables from the object\\n- Pass the prompt to OpenAI\\n- Parse the OpenAI response as a string\\n\",\n",
      "  \"code\": \"tags: [langchain, rag]\\ndescription: |\\n    Create a chain that does the following:\\n    - Accept a string as input\\n    - Retrieve matching documents from a DocArrayInMemorySearch vectorstore, and pass through the results and the original question to a prompt\\n    - Format the prompt using variables from the object\\n    - Pass the prompt to OpenAI\\n    - Parse the OpenAI response as a string\\n\\\"\\\"\\\"\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.schema.output_parser import StrOutputParser\\nfrom langchain.schema.runnable import RunnableParallel, RunnablePassthrough\\nfrom langchain.vectorstores import DocArrayInMemorySearch\\n\\nvectorstore = DocArrayInMemorySearch.from_texts(\\n    [\\\"harrison worked at kensho\\\", \\\"bears like to eat honey\\\"],\\n    embedding=OpenAIEmbeddings(),\\n)\\nretriever = vectorstore.as_retriever()\\n\\ntemplate = \\\"\\\"\\\"Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n\\\"\\\"\\\"\\nprompt = ChatPromptTemplate.from_template(template)\\nmodel = ChatOpenAI()\\noutput_parser = StrOutputParser()\\n\\nsetup_and_retrieval = RunnableParallel(\\n    {\\\"context\\\": retriever, \\\"question\\\": RunnablePassthrough()}\\n)\\n\\nchain = setup_and_retrieval | prompt | model | output_parser\\n\\nchain.invoke(\\\"where did harrison work?\\\")\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "# Define the path to the data directory\n",
    "DATA_DIR = \"../data\"\n",
    "\n",
    "# Define a function to extract tags, description, and code from a file\n",
    "def extract_information(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "        \n",
    "        # Initialize the data dictionary\n",
    "        data = {\n",
    "            'file_name': os.path.basename(file_path),\n",
    "            'tags': None,\n",
    "            'description': None,\n",
    "            'code': None\n",
    "        }\n",
    "\n",
    "        # Define the regular expression pattern\n",
    "        pattern = r'\"\"\"(.*?)\"\"\"'\n",
    "\n",
    "        # Extract the YAML block\n",
    "        match = re.search(pattern, content, re.DOTALL)\n",
    "        yaml_block = match.group(1)\n",
    "\n",
    "        # Parse the YAML block\n",
    "        metadata = yaml.safe_load(yaml_block)\n",
    "        \n",
    "        # Regular expressions to match tags and description\n",
    "        data['tags'] = metadata['tags']\n",
    "        data['description'] = metadata['description']\n",
    "        \n",
    "        # Extract code\n",
    "        code_start = content.find(\"# ---\", content.rfind(\"# ---\")) + len(\"# ---\")\n",
    "        if code_start != -1:\n",
    "            data['code'] = content[code_start:].strip()\n",
    "        \n",
    "        # Check for missing data\n",
    "        missing_data = [key for key, value in data.items() if value is None]\n",
    "        if missing_data:\n",
    "            print(f\"Error in file {data['file_name']}: Missing data for {', '.join(missing_data)}.\")\n",
    "        \n",
    "        return data\n",
    "\n",
    "# Define the main function to iterate through the files and extract information\n",
    "def main():\n",
    "    for file_name in os.listdir(DATA_DIR):\n",
    "        # Check if the file is a Python file and not a subdirectory\n",
    "        if file_name.endswith('.py') and os.path.isfile(os.path.join(DATA_DIR, file_name)):\n",
    "            file_path = os.path.join(DATA_DIR, file_name)\n",
    "            file_data = extract_information(file_path)\n",
    "            print(json.dumps(file_data, indent=2))\n",
    "\n",
    "# Run the main function\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
